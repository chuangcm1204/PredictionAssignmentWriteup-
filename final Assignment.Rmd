---
title: "FinalAssignment - Practical Machine Learning Coursera"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement ¡V a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).



## Data

The training data for this project are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv. 

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. If you use the document you create for this class for any purpose please cite them as they have been very generous in allowing their data to be used for this kind of assignment.


## Data Analysis

We download the dataset from above internet to local for analysis.

## Load the required package and library

install.packages("caret")

install.packages("rattle")

install.packages("rpart")

install.packages("rpart.plot")

install.packages("randomForest")

install.packages("repmis")

install.packages("e1071")

library(caret)

library(rattle)

library(rpart)

library(rpart.plot)

library(randomForest)

library(repmis)

library(e1071)


# Download the training and testing data

training <- read.csv("pml-training.csv", na.strings = c("NA", ""))

testing <- read.csv("pml-testing.csv", na.strings = c("NA", ""))

#Data clean for traing and testing data. 
Removing the missing value fot training and test data.


training <- training[, colSums(is.na(training)) == 0]

testing <- testing[, colSums(is.na(testing)) == 0]

Removing the first few columns of the data

trainData <- training[, -c(1:7)]

testData <- testing[, -c(1:7)]

# Partition the data to training and validation data for cross validation

Separate the training dataset to training set(70%) and validata set(30%)

set.seed(7878) 

inTrain <- createDataPartition(trainData$classe, p = 0.7, list = FALSE)

train <- trainData[inTrain, ]

validation <- trainData[-inTrain, ]

# Predict with Decision Tree Model using 10-fold out of sample testing

control <- trainControl(method = "cv", number = 10)

fitRpart <- train(classe ~ ., data = train, method = "rpart", trControl = control)

print(fitRpart, digits = 4)

# predict outcomes using validation set

predictRpart <- predict(fitRpart, validation)

fancyRpartPlot(fitRpart$finalModel)

# Confusion Matrix and show prediction result
Because the accuracy of decision tree is too low(accuracy=0.494). The out of sample error estimation is 50.6%. Therefore, we should try other method to fit the model.

confRpart <- confusionMatrix(valid$classe, predictRpart)


accuracyRpart <- confRpart$overall[1]

## Using Random Forest method
Try random forest method to build the model and predict the result

fitRf <- train(classe ~ ., data = train, method = "rf", trControl = control)

print(fitRf, digits = 4)

# predict outcomes using validation set by 10-fold out of sample testing

predictRf <- predict(fitRf, validation)


confRf <- confusionMatrix(validation$classe, predictRf)


accuracyRf <- confRf$overall[1]

Because the accuracy of random forest is good(accuracy=0.994). The out of sample error estimation is 0.6%. Therefore, we could use random forest method to fit the model with test data.

predict(fitRf, testData)

